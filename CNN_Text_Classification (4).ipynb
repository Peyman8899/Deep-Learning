{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUERznfNzj2A"
      },
      "source": [
        "## CNN-Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split, Subset, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torchvision\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from numpy.random import seed\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ypq85rMLUu-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmkhAZoUzj2B",
        "outputId": "f3e0bf32-d5e2-477d-f42f-b78d64fc0191"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Emotion\n",
              "0                            i didnt feel humiliated  sadness\n",
              "1  i can go from feeling so hopeless to so damned...  sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong    anger\n",
              "3  i am ever feeling nostalgic about the fireplac...     love\n",
              "4                               i am feeling grouchy    anger"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "21459"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"Emotion_final.csv\")\n",
        "data.head()\n",
        "len(data)\n",
        "\n",
        "data = shuffle(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqGQzzwOzj2C",
        "outputId": "d4dc5f81-0a4f-4a00-b27f-2fc59eafaae8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11319</th>\n",
              "      <td>i feel impatient with the christian church dis...</td>\n",
              "      <td>anger</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>i feel like him try to stay as faithful as pos...</td>\n",
              "      <td>happy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11516</th>\n",
              "      <td>im feeling pretty proud most of the elements i...</td>\n",
              "      <td>happy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18681</th>\n",
              "      <td>i went to bed feeling pretty proud of myself e...</td>\n",
              "      <td>happy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2273</th>\n",
              "      <td>i feel heartbroken mostly for my daughter and ...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  Emotion  label\n",
              "11319  i feel impatient with the christian church dis...    anger      2\n",
              "793    i feel like him try to stay as faithful as pos...    happy      1\n",
              "11516  im feeling pretty proud most of the elements i...    happy      1\n",
              "18681  i went to bed feeling pretty proud of myself e...    happy      1\n",
              "2273   i feel heartbroken mostly for my daughter and ...  sadness      4"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "1    7029\n",
              "4    6265\n",
              "2    2993\n",
              "0    2652\n",
              "5    1641\n",
              "3     879\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Map text label to numbers\n",
        "\n",
        "label_map = {'fear':0, 'happy':1, 'anger':2, 'surprise':3, 'sadness':4, 'love':5}\n",
        "data[\"label\"] = data.Emotion.apply(lambda x: label_map[x])\n",
        "data.head()\n",
        "data.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK8783SGzj2D"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training, evaluation, and test\n",
        "\n",
        "train_data = data.iloc[0: 15000]\n",
        "eval_data = data.iloc[15000: 18000]\n",
        "test_data = data.iloc[18000: ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGn4Xm95zj2D"
      },
      "source": [
        "### Q3.1. Preprocess Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBc7beiFzj2F",
        "outputId": "703ced12-0691-42ea-adc4-062de00d2c7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "15959"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "0    14985\n",
              "1      486\n",
              "2      165\n",
              "3       70\n",
              "4       48\n",
              "6       27\n",
              "7       20\n",
              "5       18\n",
              "9       12\n",
              "8        9\n",
              "dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# A few exploration steps\n",
        "from tensorflow import keras\n",
        "\n",
        "# check document length\n",
        "(train_data.apply(lambda row: len(row[\"Text\"].split()), axis = 1)).hist(bins = 200).plot()\n",
        "#(train_data.apply(lambda row: len(row[\"title\"].split()), axis = 1)).hist(bins = 200).plot()\n",
        "\n",
        "# check word frequencies\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "#tokenizer.fit_on_texts(train_data[\"title\"] +\" \"+ train_data[\"text\"])\n",
        "tokenizer.fit_on_texts(train_data[\"Text\"])\n",
        "\n",
        "len(tokenizer.word_counts)\n",
        "word_counts = pd.Series(list(tokenizer.word_counts.values()))\n",
        "(word_counts//30).value_counts().sort_values(ascending = False)[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2d1gPiRzj2H"
      },
      "outputs": [],
      "source": [
        "class News_dataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = torch.Tensor(x).long()\n",
        "        self.y = torch.Tensor(y).long()\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.x.size()[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQGxKIdqzj2I"
      },
      "outputs": [],
      "source": [
        "def preprocess(train_data, eval_data, test_data, vocab_size = 10000, max_sent = 50):\n",
        "    \n",
        "    tokenizer = keras.preprocessing.text.Tokenizer(num_words = vocab_size)\n",
        "    tokenizer.fit_on_texts(train_data[\"Text\"])\n",
        "\n",
        "    train_seq = tokenizer.texts_to_sequences(train_data[\"Text\"])\n",
        "    train_seq = keras.preprocessing.sequence.pad_sequences(train_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=max_sent)\n",
        "\n",
        "    eval_seq = tokenizer.texts_to_sequences(eval_data[\"Text\"])\n",
        "    eval_seq = keras.preprocessing.sequence.pad_sequences(eval_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=max_sent)\n",
        "\n",
        "    \n",
        "    test_seq = tokenizer.texts_to_sequences(test_data[\"Text\"])\n",
        "    test_seq = keras.preprocessing.sequence.pad_sequences(test_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=max_sent)\n",
        "\n",
        "    # dataset\n",
        "    train_dataset = News_dataset(train_seq, train_data[\"label\"].values)\n",
        "    eval_dataset = News_dataset(eval_seq, eval_data[\"label\"].values)\n",
        "    test_dataset = News_dataset(test_seq, test_data[\"label\"].values)\n",
        "    \n",
        "    return train_dataset, eval_dataset, test_dataset, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY5ZEQ20zj2J"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "max_sent = 50\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset, tokenizer = preprocess(train_data, eval_data, test_data, \\\n",
        "                                                       vocab_size, max_sent )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMsugD2vzj2K"
      },
      "outputs": [],
      "source": [
        "#tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcEsWPE7zj2L"
      },
      "source": [
        "### Load pretrained word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0osLDWJuzj2M"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAQEFm69zj2M",
        "outputId": "55c62451-e4fc-42e6-96e0-78768f7ba809"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.34MB/s]                             \n",
            "100%|███████████████████████████████▉| 399999/400000 [00:06<00:00, 64227.45it/s]\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "emb_dim = 100\n",
        "\n",
        "vector = torchtext.vocab.GloVe(name='6B', dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af1r7pTnzj2M",
        "outputId": "0990b743-4a97-4fcf-d9e8-19b35cf5f18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "missing_words = []  # check if any word without a vector\n",
        "\n",
        "# initialize embedding matrix\n",
        "emb_weight = np.zeros((vocab_size, emb_dim))\n",
        "\n",
        "# loop through all words\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "\n",
        "  # align with word index in sentences, since the first 3 indexes are reserved\n",
        "  if idx < vocab_size :  \n",
        "    try:\n",
        "      emb = vector[word]\n",
        "      emb_weight[idx] = emb    \n",
        "\n",
        "    # not every word has a vector\n",
        "    except:\n",
        "      missing_words.append(word)\n",
        "    \n",
        "print(missing_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rLwSpKazj2N",
        "outputId": "c10c40f6-48b5-4dd8-f3b8-145ad83e596e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 100])\n"
          ]
        }
      ],
      "source": [
        "emb_matrix = torch.Tensor(emb_weight)\n",
        "print(emb_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mftMr6gzj2O"
      },
      "source": [
        "### 3.3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4yHzEjEzj2P"
      },
      "outputs": [],
      "source": [
        "class cnn_text(nn.Module):\n",
        "\n",
        "    # define all the layers used in model\n",
        "    def __init__(self, vocab_size, emb_dim, num_classes, num_filters,  \\\n",
        "                 kernel_sizes, emb_weight = None, freeze = False, dropout_rate = 0.5):\n",
        "      \n",
        "        super(cnn_text, self).__init__()\n",
        "        \n",
        "        self.emb_dim = emb_dim\n",
        "        \n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.num_classes = num_classes\n",
        "       \n",
        "        if emb_weight is None:      \n",
        "            self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding.from_pretrained(emb_weight, freeze=freeze)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(self.emb_dim, self.num_filters, f) for f in self.kernel_sizes])\n",
        "        self.fc = nn.Linear(len(kernel_sizes)*self.num_filters, self.num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embedding(x) # Batch x sent_len x emb_dim\n",
        "        \n",
        "        x = torch.swapaxes(x, 1, 2) # Batch x emb_dim x max_sent \n",
        "\n",
        "        x = [F.relu(conv(x)) for conv in self.convs]  # output of three conv, batch x num_filter x L\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
        "\n",
        "        x = torch.cat(x, 1)  \n",
        "\n",
        "        x = self.dropout(x)  \n",
        "\n",
        "        logit = self.fc(x)  \n",
        "\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2wCgzzzj2Q"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1TN_Pojzj2Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_model(model, train_dataset, eval_dataset, test_dataset, device, \n",
        "                optimizer, epochs=30, batch_size=64):\n",
        "    \n",
        "    # construct dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # history\n",
        "    history = {'train_loss': [],\n",
        "               'train_acc': [],\n",
        "               'eval_loss': [],\n",
        "               'eval_acc': [],\n",
        "               'test_acc': 0}\n",
        "    \n",
        "    patience = 5\n",
        "    max_acc = -np.Inf\n",
        "    cnt = 0\n",
        "    \n",
        "    # setup loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "    # training loop\n",
        "    print('Training Start')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        \n",
        "        for x, y in train_loader:\n",
        "            \n",
        "            # move data to device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            # forward\n",
        "            outputs = model(x)  # (num_batch, 45) \n",
        "            _, pred = torch.max(outputs, dim = -1)\n",
        "            cur_train_loss = criterion(outputs, y)\n",
        "            cur_train_acc = (pred == y).float().mean().item()\n",
        "            \n",
        "            # backward\n",
        "            cur_train_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # loss and acc\n",
        "            train_loss += cur_train_loss\n",
        "            train_acc += cur_train_acc\n",
        "\n",
        "        # test start\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in eval_loader:\n",
        "                # move\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                # predict\n",
        "                \n",
        "                outputs = model(x)  # (num_batch, 45) \n",
        "                _, pred = torch.max(outputs, dim = -1)\n",
        "                cur_test_loss = criterion(outputs, y)\n",
        "                cur_test_acc = (pred == y).float().mean().item() \n",
        "                # loss and acc\n",
        "                test_loss += cur_test_loss\n",
        "                test_acc += cur_test_acc\n",
        "\n",
        "        # epoch output\n",
        "        train_loss = (train_loss/len(train_loader)).item()\n",
        "        train_acc = train_acc/len(train_loader)\n",
        "        val_loss = (test_loss/len(eval_loader)).item()\n",
        "        val_acc = test_acc/len(eval_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['eval_loss'].append(val_loss)\n",
        "        history['eval_acc'].append(val_acc)\n",
        "        print(f\"Epoch:{epoch + 1} / {epochs}, train loss:{train_loss:.4f},\\\n",
        "        train_acc:{train_acc:.4f}, valid loss:{val_loss:.4f} valid acc:{val_acc:.4f}\")\n",
        "        \n",
        "        if test_acc > max_acc:\n",
        "            max_acc = test_acc\n",
        "            cnt = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"model saved!\")\n",
        "        else:\n",
        "            cnt += 1\n",
        "            \n",
        "            if cnt == patience:\n",
        "                print(\"early stopping!\")\n",
        "                break\n",
        "    \n",
        "    # restore best model\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSBj_I3Nzj2S"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_dataset, batch_size = 64):    # get test performance\n",
        "    \n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    test_acc = 0\n",
        "    ys = []\n",
        "    preds = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            # move\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            # predict\n",
        "            outputs = model(x)\n",
        "            _, pred = torch.max(outputs, dim = -1)\n",
        "            \n",
        "            ys.append(y.cpu().numpy())\n",
        "            preds.append(pred.cpu().numpy())\n",
        "    \n",
        "    ys = np.concatenate(ys)\n",
        "    preds = np.concatenate(preds)\n",
        "    \n",
        "    test_acc = (preds == ys).astype(int).mean()\n",
        "    print(classification_report(ys, preds))\n",
        "        \n",
        "    print(f\"Test acc: {test_acc:.4f}\")\n",
        "    \n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsHM5HZdzj2T"
      },
      "source": [
        "### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU-AYOSYzj2T"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzCuW2Oszj2U"
      },
      "outputs": [],
      "source": [
        "kernel_sizes = [1, 3, 5]\n",
        "num_filters = 30\n",
        "num_classes = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuWWuhjqzj2V",
        "outputId": "252cec69-5ced-448e-f66c-5506e94cdf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Start\n",
            "Epoch:1 / 100, train loss:1.6710,        train_acc:0.3111, valid loss:1.5250 valid acc:0.4027\n",
            "model saved!\n",
            "Epoch:2 / 100, train loss:1.5506,        train_acc:0.3746, valid loss:1.4777 valid acc:0.4435\n",
            "model saved!\n",
            "Epoch:3 / 100, train loss:1.4693,        train_acc:0.4206, valid loss:1.3952 valid acc:0.4840\n",
            "model saved!\n",
            "Epoch:4 / 100, train loss:1.3711,        train_acc:0.4806, valid loss:1.2609 valid acc:0.5540\n",
            "model saved!\n",
            "Epoch:5 / 100, train loss:1.2243,        train_acc:0.5505, valid loss:1.0681 valid acc:0.6464\n",
            "model saved!\n",
            "Epoch:6 / 100, train loss:1.0456,        train_acc:0.6287, valid loss:0.8749 valid acc:0.7203\n",
            "model saved!\n",
            "Epoch:7 / 100, train loss:0.8672,        train_acc:0.6977, valid loss:0.6988 valid acc:0.7660\n",
            "model saved!\n",
            "Epoch:8 / 100, train loss:0.7045,        train_acc:0.7680, valid loss:0.5657 valid acc:0.8267\n",
            "model saved!\n",
            "Epoch:9 / 100, train loss:0.5969,        train_acc:0.8032, valid loss:0.4825 valid acc:0.8461\n",
            "model saved!\n",
            "Epoch:10 / 100, train loss:0.4875,        train_acc:0.8382, valid loss:0.4211 valid acc:0.8631\n",
            "model saved!\n",
            "Epoch:11 / 100, train loss:0.4298,        train_acc:0.8564, valid loss:0.3808 valid acc:0.8738\n",
            "model saved!\n",
            "Epoch:12 / 100, train loss:0.3826,        train_acc:0.8735, valid loss:0.3526 valid acc:0.8812\n",
            "model saved!\n",
            "Epoch:13 / 100, train loss:0.3299,        train_acc:0.8912, valid loss:0.3325 valid acc:0.8853\n",
            "model saved!\n",
            "Epoch:14 / 100, train loss:0.3032,        train_acc:0.8989, valid loss:0.3179 valid acc:0.8893\n",
            "model saved!\n",
            "Epoch:15 / 100, train loss:0.2708,        train_acc:0.9107, valid loss:0.3011 valid acc:0.8940\n",
            "model saved!\n",
            "Epoch:16 / 100, train loss:0.2541,        train_acc:0.9155, valid loss:0.2891 valid acc:0.8989\n",
            "model saved!\n",
            "Epoch:17 / 100, train loss:0.2351,        train_acc:0.9221, valid loss:0.2871 valid acc:0.8983\n",
            "Epoch:18 / 100, train loss:0.2099,        train_acc:0.9303, valid loss:0.2839 valid acc:0.9004\n",
            "model saved!\n",
            "Epoch:19 / 100, train loss:0.1968,        train_acc:0.9375, valid loss:0.2790 valid acc:0.9027\n",
            "model saved!\n",
            "Epoch:20 / 100, train loss:0.1837,        train_acc:0.9389, valid loss:0.2752 valid acc:0.9027\n",
            "Epoch:21 / 100, train loss:0.1712,        train_acc:0.9440, valid loss:0.2791 valid acc:0.9027\n",
            "model saved!\n",
            "Epoch:22 / 100, train loss:0.1609,        train_acc:0.9467, valid loss:0.2792 valid acc:0.9031\n",
            "model saved!\n",
            "Epoch:23 / 100, train loss:0.1510,        train_acc:0.9514, valid loss:0.2745 valid acc:0.9071\n",
            "model saved!\n",
            "Epoch:24 / 100, train loss:0.1441,        train_acc:0.9549, valid loss:0.2761 valid acc:0.9037\n",
            "Epoch:25 / 100, train loss:0.1363,        train_acc:0.9555, valid loss:0.2752 valid acc:0.9057\n",
            "Epoch:26 / 100, train loss:0.1279,        train_acc:0.9582, valid loss:0.2807 valid acc:0.9041\n",
            "Epoch:27 / 100, train loss:0.1160,        train_acc:0.9616, valid loss:0.2783 valid acc:0.9088\n",
            "model saved!\n",
            "Epoch:28 / 100, train loss:0.1157,        train_acc:0.9620, valid loss:0.2773 valid acc:0.9061\n",
            "Epoch:29 / 100, train loss:0.1104,        train_acc:0.9647, valid loss:0.2835 valid acc:0.9048\n",
            "Epoch:30 / 100, train loss:0.1102,        train_acc:0.9643, valid loss:0.2846 valid acc:0.9048\n",
            "Epoch:31 / 100, train loss:0.1026,        train_acc:0.9672, valid loss:0.2888 valid acc:0.9018\n",
            "Epoch:32 / 100, train loss:0.0923,        train_acc:0.9702, valid loss:0.2919 valid acc:0.9005\n",
            "early stopping!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87       446\n",
            "           1       0.91      0.94      0.92      1100\n",
            "           2       0.91      0.87      0.89       490\n",
            "           3       0.82      0.78      0.80       153\n",
            "           4       0.92      0.95      0.94       997\n",
            "           5       0.82      0.77      0.79       273\n",
            "\n",
            "    accuracy                           0.90      3459\n",
            "   macro avg       0.88      0.86      0.87      3459\n",
            "weighted avg       0.90      0.90      0.90      3459\n",
            "\n",
            "Test acc: 0.9008\n"
          ]
        }
      ],
      "source": [
        "# Case A, randomly initialize embedding layer\n",
        "\n",
        "model_cnn_a = cnn_text(vocab_size, emb_dim, num_classes, num_filters, \\\n",
        "                 kernel_sizes, emb_weight = None)\n",
        "                  \n",
        "#summary(model_cnn, (32, max_sent))\n",
        "\n",
        "optimizer = torch.optim.Adam(model_cnn_a.parameters(), lr=0.0005)\n",
        "\n",
        "model_cnn_a, hista = train_model(model_cnn_a, train_dataset, eval_dataset, test_dataset, \\\n",
        "                           device, optimizer, epochs=100)\n",
        "\n",
        "test_acc = test_model(model_cnn_a, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjAot-bPzj2W",
        "outputId": "1101c3e0-709a-44aa-dcac-5d08efc87927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Start\n",
            "Epoch:1 / 100, train loss:1.5094,        train_acc:0.4218, valid loss:1.2683 valid acc:0.5615\n",
            "model saved!\n",
            "Epoch:2 / 100, train loss:1.1600,        train_acc:0.5914, valid loss:0.9649 valid acc:0.6811\n",
            "model saved!\n",
            "Epoch:3 / 100, train loss:0.9656,        train_acc:0.6575, valid loss:0.8122 valid acc:0.7456\n",
            "model saved!\n",
            "Epoch:4 / 100, train loss:0.8443,        train_acc:0.6988, valid loss:0.7100 valid acc:0.7723\n",
            "model saved!\n",
            "Epoch:5 / 100, train loss:0.7718,        train_acc:0.7269, valid loss:0.6443 valid acc:0.7976\n",
            "model saved!\n",
            "Epoch:6 / 100, train loss:0.7101,        train_acc:0.7535, valid loss:0.5920 valid acc:0.8102\n",
            "model saved!\n",
            "Epoch:7 / 100, train loss:0.6714,        train_acc:0.7679, valid loss:0.5550 valid acc:0.8199\n",
            "model saved!\n",
            "Epoch:8 / 100, train loss:0.6307,        train_acc:0.7814, valid loss:0.5366 valid acc:0.8286\n",
            "model saved!\n",
            "Epoch:9 / 100, train loss:0.6075,        train_acc:0.7893, valid loss:0.5080 valid acc:0.8359\n",
            "model saved!\n",
            "Epoch:10 / 100, train loss:0.5905,        train_acc:0.7927, valid loss:0.4927 valid acc:0.8376\n",
            "model saved!\n",
            "Epoch:11 / 100, train loss:0.5652,        train_acc:0.8009, valid loss:0.4752 valid acc:0.8453\n",
            "model saved!\n",
            "Epoch:12 / 100, train loss:0.5441,        train_acc:0.8150, valid loss:0.4643 valid acc:0.8474\n",
            "model saved!\n",
            "Epoch:13 / 100, train loss:0.5309,        train_acc:0.8142, valid loss:0.4523 valid acc:0.8489\n",
            "model saved!\n",
            "Epoch:14 / 100, train loss:0.5202,        train_acc:0.8225, valid loss:0.4392 valid acc:0.8476\n",
            "Epoch:15 / 100, train loss:0.5045,        train_acc:0.8296, valid loss:0.4318 valid acc:0.8577\n",
            "model saved!\n",
            "Epoch:16 / 100, train loss:0.4947,        train_acc:0.8323, valid loss:0.4207 valid acc:0.8577\n",
            "Epoch:17 / 100, train loss:0.4806,        train_acc:0.8336, valid loss:0.4164 valid acc:0.8577\n",
            "Epoch:18 / 100, train loss:0.4723,        train_acc:0.8363, valid loss:0.4077 valid acc:0.8617\n",
            "model saved!\n",
            "Epoch:19 / 100, train loss:0.4591,        train_acc:0.8442, valid loss:0.4026 valid acc:0.8685\n",
            "model saved!\n",
            "Epoch:20 / 100, train loss:0.4482,        train_acc:0.8422, valid loss:0.4013 valid acc:0.8624\n",
            "Epoch:21 / 100, train loss:0.4440,        train_acc:0.8470, valid loss:0.3901 valid acc:0.8715\n",
            "model saved!\n",
            "Epoch:22 / 100, train loss:0.4366,        train_acc:0.8511, valid loss:0.3898 valid acc:0.8698\n",
            "Epoch:23 / 100, train loss:0.4272,        train_acc:0.8524, valid loss:0.3877 valid acc:0.8668\n",
            "Epoch:24 / 100, train loss:0.4156,        train_acc:0.8606, valid loss:0.3745 valid acc:0.8697\n",
            "Epoch:25 / 100, train loss:0.4104,        train_acc:0.8607, valid loss:0.3824 valid acc:0.8728\n",
            "model saved!\n",
            "Epoch:26 / 100, train loss:0.4063,        train_acc:0.8632, valid loss:0.3704 valid acc:0.8748\n",
            "model saved!\n",
            "Epoch:27 / 100, train loss:0.3858,        train_acc:0.8688, valid loss:0.3691 valid acc:0.8758\n",
            "model saved!\n",
            "Epoch:28 / 100, train loss:0.3975,        train_acc:0.8648, valid loss:0.3629 valid acc:0.8701\n",
            "Epoch:29 / 100, train loss:0.3897,        train_acc:0.8679, valid loss:0.3666 valid acc:0.8758\n",
            "Epoch:30 / 100, train loss:0.3866,        train_acc:0.8688, valid loss:0.3596 valid acc:0.8791\n",
            "model saved!\n",
            "Epoch:31 / 100, train loss:0.3830,        train_acc:0.8701, valid loss:0.3599 valid acc:0.8805\n",
            "model saved!\n",
            "Epoch:32 / 100, train loss:0.3700,        train_acc:0.8721, valid loss:0.3551 valid acc:0.8774\n",
            "Epoch:33 / 100, train loss:0.3620,        train_acc:0.8760, valid loss:0.3532 valid acc:0.8725\n",
            "Epoch:34 / 100, train loss:0.3586,        train_acc:0.8765, valid loss:0.3484 valid acc:0.8778\n",
            "Epoch:35 / 100, train loss:0.3537,        train_acc:0.8802, valid loss:0.3492 valid acc:0.8792\n",
            "Epoch:36 / 100, train loss:0.3520,        train_acc:0.8789, valid loss:0.3487 valid acc:0.8801\n",
            "early stopping!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.81       446\n",
            "           1       0.89      0.90      0.89      1100\n",
            "           2       0.87      0.82      0.85       490\n",
            "           3       0.76      0.74      0.75       153\n",
            "           4       0.86      0.93      0.89       997\n",
            "           5       0.80      0.82      0.81       273\n",
            "\n",
            "    accuracy                           0.86      3459\n",
            "   macro avg       0.84      0.83      0.83      3459\n",
            "weighted avg       0.86      0.86      0.86      3459\n",
            "\n",
            "Test acc: 0.8641\n"
          ]
        }
      ],
      "source": [
        "# Case B: Use word vectors and freeze word vectors\n",
        "model_cnn_b = cnn_text(vocab_size, emb_dim, num_classes, num_filters, \\\n",
        "                 kernel_sizes, emb_weight = emb_matrix, freeze = True)\n",
        "                  \n",
        "#summary(model_cnn, (32, max_sent))\n",
        "\n",
        "optimizer = torch.optim.Adam(model_cnn_b.parameters(), lr=0.0005)\n",
        "\n",
        "model_cnn_b, hist_b = train_model(model_cnn_b, train_dataset, eval_dataset, test_dataset, \\\n",
        "                           device, optimizer, epochs=100)\n",
        "\n",
        "test_acc = test_model(model_cnn_b, test_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuqFp_RTzj2X",
        "outputId": "7d5e67de-c8dd-4f2e-b6c6-730069c50985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Start\n",
            "Epoch:1 / 100, train loss:1.5117,        train_acc:0.4181, valid loss:1.2247 valid acc:0.5740\n",
            "model saved!\n",
            "Epoch:2 / 100, train loss:0.9903,        train_acc:0.6664, valid loss:0.6625 valid acc:0.8020\n",
            "model saved!\n",
            "Epoch:3 / 100, train loss:0.5753,        train_acc:0.8143, valid loss:0.3806 valid acc:0.8741\n",
            "model saved!\n",
            "Epoch:4 / 100, train loss:0.3875,        train_acc:0.8732, valid loss:0.2774 valid acc:0.8985\n",
            "model saved!\n",
            "Epoch:5 / 100, train loss:0.3078,        train_acc:0.8975, valid loss:0.2338 valid acc:0.9101\n",
            "model saved!\n",
            "Epoch:6 / 100, train loss:0.2586,        train_acc:0.9118, valid loss:0.2127 valid acc:0.9138\n",
            "model saved!\n",
            "Epoch:7 / 100, train loss:0.2258,        train_acc:0.9195, valid loss:0.2023 valid acc:0.9138\n",
            "Epoch:8 / 100, train loss:0.2136,        train_acc:0.9269, valid loss:0.2001 valid acc:0.9158\n",
            "model saved!\n",
            "Epoch:9 / 100, train loss:0.1884,        train_acc:0.9322, valid loss:0.1946 valid acc:0.9168\n",
            "model saved!\n",
            "Epoch:10 / 100, train loss:0.1785,        train_acc:0.9365, valid loss:0.1978 valid acc:0.9149\n",
            "Epoch:11 / 100, train loss:0.1693,        train_acc:0.9382, valid loss:0.1987 valid acc:0.9182\n",
            "model saved!\n",
            "Epoch:12 / 100, train loss:0.1523,        train_acc:0.9443, valid loss:0.1960 valid acc:0.9176\n",
            "Epoch:13 / 100, train loss:0.1466,        train_acc:0.9481, valid loss:0.1965 valid acc:0.9175\n",
            "Epoch:14 / 100, train loss:0.1323,        train_acc:0.9539, valid loss:0.1973 valid acc:0.9195\n",
            "model saved!\n",
            "Epoch:15 / 100, train loss:0.1293,        train_acc:0.9553, valid loss:0.1977 valid acc:0.9179\n",
            "Epoch:16 / 100, train loss:0.1180,        train_acc:0.9574, valid loss:0.2029 valid acc:0.9152\n",
            "Epoch:17 / 100, train loss:0.1135,        train_acc:0.9585, valid loss:0.2015 valid acc:0.9202\n",
            "model saved!\n",
            "Epoch:18 / 100, train loss:0.1009,        train_acc:0.9641, valid loss:0.2024 valid acc:0.9205\n",
            "model saved!\n",
            "Epoch:19 / 100, train loss:0.1009,        train_acc:0.9659, valid loss:0.2093 valid acc:0.9152\n",
            "Epoch:20 / 100, train loss:0.0934,        train_acc:0.9681, valid loss:0.2119 valid acc:0.9176\n",
            "Epoch:21 / 100, train loss:0.0896,        train_acc:0.9697, valid loss:0.2160 valid acc:0.9188\n",
            "Epoch:22 / 100, train loss:0.0832,        train_acc:0.9717, valid loss:0.2211 valid acc:0.9155\n",
            "Epoch:23 / 100, train loss:0.0785,        train_acc:0.9726, valid loss:0.2187 valid acc:0.9182\n",
            "early stopping!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.88       446\n",
            "           1       0.94      0.94      0.94      1100\n",
            "           2       0.93      0.91      0.92       490\n",
            "           3       0.79      0.78      0.79       153\n",
            "           4       0.94      0.96      0.95       997\n",
            "           5       0.81      0.86      0.84       273\n",
            "\n",
            "    accuracy                           0.92      3459\n",
            "   macro avg       0.89      0.89      0.89      3459\n",
            "weighted avg       0.92      0.92      0.92      3459\n",
            "\n",
            "Test acc: 0.9188\n"
          ]
        }
      ],
      "source": [
        "# Case C: Use word vectors and allow fine tuning\n",
        "\n",
        "model_cnn_c = cnn_text(vocab_size, emb_dim, num_classes, num_filters, \\\n",
        "                 kernel_sizes, emb_weight = emb_matrix, freeze = False)\n",
        "                  \n",
        "#summary(model_cnn, (32, max_sent))\n",
        "\n",
        "optimizer = torch.optim.Adam(model_cnn_c.parameters(), lr=0.0005)\n",
        "\n",
        "model_cnn_c, hist_c = train_model(model_cnn_c, train_dataset, eval_dataset, test_dataset, \\\n",
        "                           device, optimizer, epochs=100)\n",
        "\n",
        "test_acc = test_model(model_cnn_c, test_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx0-Hg2mzj2Y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED4_FCBezj2Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "CNN-Text Classification.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}